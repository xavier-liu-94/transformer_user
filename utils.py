from torch.utils.tensorboard import SummaryWriter
from torch.utils.hooks import RemovableHandle
import torch
from typing import List
import torch.nn as nn
from typing import Dict, Optional
import os


class TorchTrainingVisualizer:
    """
    ğŸ¯ æ¯æ¬¡è°ƒç”¨ log_metrics() å¿…é¡»æ‰“ä¸€ä¸ªç‚¹ï¼ä¸å¹³å‡ã€ä¸è·³è¿‡ã€ä¸é™é‡‡æ ·ï¼
    ä¸“ä¸ºè°ƒè¯•è®­ç»ƒåŠ¨æ€è®¾è®¡ï¼šæ¢¯åº¦çˆ†ç‚¸ã€å­¦ä¹ ç‡éœ‡è¡ã€lossçªè·³ã€è¿‡æ‹Ÿåˆè‹—å¤´...
    100% åŸå§‹æ•°æ®è®°å½•ï¼ŒTensorBoard æ˜¾ç¤ºçœŸå®è®­ç»ƒå¿ƒè·³ã€‚
    """

    def __init__(self,
                 log_dir: str = "./runs",
                 comment: str = "",
                 flush_secs: int = 10):
        """
        :param log_dir: TensorBoard æ—¥å¿—ç›®å½•
        :param comment: å®éªŒåï¼ˆè‡ªåŠ¨æ‹¼åœ¨ log_dir åï¼‰
        :param flush_secs: æ¯éš”å¤šå°‘ç§’åˆ·ç›˜ï¼ˆé»˜è®¤10ç§’ï¼Œé¿å…é¢‘ç¹IOï¼‰
        """
        self.log_dir = os.path.join(log_dir, comment) if comment else log_dir
        self.writer = SummaryWriter(log_dir=self.log_dir, flush_secs=flush_secs)
        self.step_count = 0  # å…¨å±€æ­¥æ•°è®¡æ•°å™¨ï¼ˆå¯é€‰ï¼Œç”¨äºè‡ªåŠ¨é€’å¢ï¼‰

    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None):
        """
        âœ… æ¯æ¬¡è°ƒç”¨ï¼Œå¿…é¡»æ‰“ä¸€ä¸ªç‚¹ï¼
        :param metrics: å­—å…¸ï¼Œå¦‚ {'loss': 0.5, 'grad_norm': 2.1, 'lr': 0.001}
        :param step: å¯é€‰ã€‚å¦‚æœä½ ä¼ äº†ï¼Œå°±ç”¨ä½ ä¼ çš„ï¼ˆå¦‚ batch_idxï¼‰ã€‚
                     å¦‚æœæ²¡ä¼ ï¼Œè‡ªåŠ¨ç”¨ self.step_count é€’å¢ï¼ˆæ¨èç”¨äºç®€å•åœºæ™¯ï¼‰
        """
        if step is None:
            step = self.step_count
            self.step_count += 1  # è‡ªåŠ¨é€’å¢ï¼Œç¡®ä¿æ¯ä¸ªè°ƒç”¨å¯¹åº”å”¯ä¸€ step

        for key, value in metrics.items():
            # âœ… å¼ºåˆ¶è½¬æ¢ä¸º floatï¼Œé¿å… int/np.float32 å¯¼è‡´é”™è¯¯
            if not isinstance(value, float):
                value = float(value)
            self.writer.add_scalar(key, value, step)

        # å¯é€‰ï¼šæ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆç”Ÿäº§ç¯å¢ƒå¯åˆ ï¼‰
        # print(f"ğŸ“Š Step {step}: {metrics}")

    def log_gradients(self, model: torch.nn.Module, step: Optional[int] = None):
        """
        è®°å½•æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦åˆ†å¸ƒï¼ˆç”¨äºæ£€æµ‹æ¢¯åº¦çˆ†ç‚¸ï¼‰
        """
        if step is None:
            step = self.step_count
            self.step_count += 1

        for name, param in model.named_parameters():
            if param.grad is not None:
                self.writer.add_histogram(f"gradients/{name}", param.grad, step)
                self.writer.add_histogram(f"weights/{name}", param.data, step)

    def log_hparams(self, hparams: Dict, metrics: Dict):
        """è®°å½•è¶…å‚æ•° + æœ€ç»ˆæŒ‡æ ‡ï¼ˆä»…åœ¨è®­ç»ƒç»“æŸæ—¶è°ƒç”¨ä¸€æ¬¡ï¼‰"""
        self.writer.add_hparams(hparams, metrics)

    def close(self):
        self.writer.close()
        print(f"âœ… TensorBoard æ—¥å¿—å·²ä¿å­˜è‡³: {self.log_dir}")
        print("ğŸ’¡ åœ¨ç»ˆç«¯è¿è¡Œ: tensorboard --logdir=./runs  æŸ¥çœ‹å®æ—¶æ›²çº¿")